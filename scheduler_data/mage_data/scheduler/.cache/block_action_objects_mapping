{"block_file": {"callbacks/transformer.py:callback:python:transformer": {"content": "if 'callback' not in globals():\n    from mage_ai.data_preparation.decorators import callback\n\n\n@callback('success')\ndef success_callback(parent_block_data, **kwargs):\n    pass\n\n\n@callback('failure')\ndef failure_callback(parent_block_data, **kwargs):\n    \"\"\"\n    The error that caused the block to fail is passed in as keyword\n    argument __error.\n    \"\"\"\n    pass\n", "file_path": "callbacks/transformer.py", "language": "python", "type": "callback", "uuid": "transformer"}, "callbacks/transform_data.py:callback:python:transform data": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n@transformer\ndef transform_data(data, *args, **kwargs):\n    # Paso 1: facturas principales\n    invoices = data.get(\"Invoice\", [])\n    df_invoices = pd.json_normalize(invoices, sep=\"_\")\n    \n    # Paso 2: l\u00edneas de cada factura\n    df_lines = pd.json_normalize(\n        invoices,\n        record_path=[\"Line\"],\n        meta=[\"Id\", \"DocNumber\"],  # claves de la factura padre\n        sep=\"_\"\n    )\n\n    # Retornamos ambos dataframes\n    return {\n        \"invoices\": df_invoices,\n        \"invoice_lines\": df_lines\n    }\n", "file_path": "callbacks/transform_data.py", "language": "python", "type": "callback", "uuid": "transform_data"}, "custom/transformer.py:custom:python:transformer": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    args: The output from any upstream parent blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your custom logic here\n\n    return {}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "custom/transformer.py", "language": "python", "type": "custom", "uuid": "transformer"}, "data_exporters/export_qb_costumerss.py:data_exporter:python:export qb costumerss": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_exporter.decorators import data_exporter\n\nfrom sqlalchemy import create_engine, types\nimport pandas as pd\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n   \n    engine = create_engine(\n        \"postgresql+psycopg2://root:root@warehouse:5432/postgres\"\n    )\n\n    results = {}\n\n    for table_name, df in data.items():\n        if not isinstance(df, pd.DataFrame) or df.empty:\n            print(f\"Tabla {table_name} vac\u00eda, se omite exportaci\u00f3n\")\n            continue\n\n        print(f\"Exportando {len(df)} filas a tabla {table_name}...\")\n\n        # Exportar con payload como JSONB\n        df.to_sql(\n            table_name,\n            engine,\n            if_exists=\"append\",  # para no borrar la tabla en cada carga\n            index=False,\n            dtype={\n                \"payload\": types.JSON,  # PostgreSQL interpretar\u00e1 como JSONB\n            }\n        )\n\n        results[table_name] = {\n            \"rows\": len(df),\n            \"columns\": len(df.columns),\n            \"status\": \"success\"\n        }\n\n    print(\"Exportaci\u00f3n completa\")\n    return results", "file_path": "data_exporters/export_qb_costumerss.py", "language": "python", "type": "data_exporter", "uuid": "export_qb_costumerss"}, "data_exporters/export_qb_invoices.py:data_exporter:python:export qb invoices": {"content": "#Exportamos datos a postgres\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nfrom sqlalchemy import create_engine, types\nimport pandas as pd\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n   \n    engine = create_engine(\n        \"postgresql+psycopg2://root:root@warehouse:5432/postgres\"\n    )\n\n    results = {}\n\n    for table_name, df in data.items():\n        if not isinstance(df, pd.DataFrame) or df.empty:\n            print(f\"Tabla {table_name} vac\u00eda, se omite exportaci\u00f3n\")\n            continue\n\n        print(f\"Exportando {len(df)} filas a tabla {table_name}...\")\n\n        # Exportar con payload como JSONB\n        df.to_sql(\n            table_name,\n            engine,\n            if_exists=\"append\",  # para no borrar la tabla en cada carga\n            index=False,\n            dtype={\n                \"payload\": types.JSON,  # PostgreSQL interpretar\u00e1 como JSONB\n            }\n        )\n\n        results[table_name] = {\n            \"rows\": len(df),\n            \"columns\": len(df.columns),\n            \"status\": \"success\"\n        }\n\n    print(\"Exportaci\u00f3n completa\")\n    return results\n", "file_path": "data_exporters/export_qb_invoices.py", "language": "python", "type": "data_exporter", "uuid": "export_qb_invoices"}, "data_exporters/export_qb_items.py:data_exporter:python:export qb items": {"content": "#Exportacion de datos a postgres\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nfrom sqlalchemy import create_engine\nimport pandas as pd\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n    Exporta los items transformados (qb_item) a PostgreSQL.\n    \"\"\"\n    df_items = data[\"qb_item\"]\n\n    print(f\"Datos originales - Items: {df_items.shape}\")\n\n    engine = create_engine(\n        \"postgresql+psycopg2://root:root@warehouse:5432/postgres\"\n    )\n\n    try:\n        print(\"Exportando tabla qb_item...\")\n        df_items.to_sql(\"qb_item\", engine, if_exists=\"replace\", index=False)\n\n        print(\" Datos de items exportados a Postgres correctamente\")\n\n        return {\n            \"rows\": len(df_items),\n            \"columns\": len(df_items.columns),\n            \"status\": \"success\"\n        }\n\n    except Exception as e:\n        print(f\" Error durante la exportaci\u00f3n: {str(e)}\")\n        raise e\n", "file_path": "data_exporters/export_qb_items.py", "language": "python", "type": "data_exporter", "uuid": "export_qb_items"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_loaders/ingest.py:data_loader:python:ingest": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Template code for loading data from any source.\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your data loading logic here\n\n    return {}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/ingest.py", "language": "python", "type": "data_loader", "uuid": "ingest"}, "data_loaders/ingest_qb_customers.py:data_loader:python:ingest qb customers": {"content": "#libreria para hacer peticiones http\nimport requests\nimport time\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime, timedelta\nimport json\n\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n#funcion para jalar datos de la API (anonima, no aparece)\ndef _fetch_qb_data(realm_id, access_token, query, base_url, minor_version):\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere una URL base y el minor_version\")\n\n    if not realm_id or not access_token:\n        raise ValueError(\"Se requiere un realm_id y un access_token\")\n\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n\n    params = {\n        'query': query,\n        'minorversion': minor_version\n    }\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n\n    #Reintentos\n    max_retries = 5\n    for i in range(max_retries):\n        try:\n            print(f'Request a la API: {url}\\nQuery: {query}')\n            response = requests.get(url, headers=headers, params=params, timeout=60)\n\n            # respuestas exitosas\n            if response.status_code == 200:\n                data = response.json()\n                print('Datos recividos de la API correctamente')\n                return data\n\n            # Dteccion de errores\n            elif response.status_code in [429, 500, 502, 503, 504]:\n                print(f\"Error {response.status_code}, reintentando ({i+1}/{max_retries})...\")\n                time.sleep(2 ** i)  # backoff exponencial\n                continue\n\n            else:\n                #si hay un error levanta la excepcion\n                response.raise_for_status()\n\n         #Error que nos da de la excepcion\n        except requests.exceptions.RequestException as e:\n            print(f\"Error en la pull de la API: {e}\")\n            time.sleep(2 ** i)\n\n    raise Exception(f\"Request fall\u00f3 despu\u00e9s de {max_retries} reintentos\")\n\ndef get_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\n        \"Accept\": \"application/json\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\"\n    }\n    auth = (client_id, client_secret)\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token\n    }\n\n    response = requests.post(url, headers=headers, data=data, auth=auth)\n    response.raise_for_status()\n    token_data = response.json()\n\n    # Devuelve solo el access_token fresco\n    return token_data['access_token']\n\ndef fetch_customers_with_pagination(realm_id, access_token, base_url, minor_version, chunk_start_str, chunk_end_str, page_size=1000):\n    customers = []\n    start_position = 1\n\n    while True:\n        query = (\n            f\"SELECT * FROM Customer \"\n            f\"WHERE Metadata.LastUpdatedTime >= '{chunk_start_str}' \"\n            f\"AND Metadata.LastUpdatedTime < '{chunk_end_str}' \"\n            f\"STARTPOSITION {start_position} MAXRESULTS {page_size}\"\n        )\n\n        data = _fetch_qb_data(realm_id, access_token, query, base_url, minor_version)\n        batch = data.get(\"QueryResponse\", {}).get(\"Customer\", [])\n\n        if not batch:\n            break\n\n        customers.extend(batch)\n        start_position += page_size\n\n        if len(batch) < page_size:\n            break\n\n    return customers\n\n\n#  Procesar backfill con segmentaci\u00f3n\ndef process_backfill_chunks(realm_id, access_token, base_url, minor_version, fecha_inicio, fecha_fin, chunk_days=7):\n    \n    start_dt = datetime.fromisoformat(fecha_inicio.replace('Z', '+00:00'))\n    end_dt = datetime.fromisoformat(fecha_fin.replace('Z', '+00:00'))\n    \n    all_customers = []\n    processing_log = []\n    \n    current_date = start_dt\n    chunk_number = 1\n    \n    print(f\"Iniciando backfill desde {fecha_inicio} hasta {fecha_fin}\")\n    print(f\"Segmentaci\u00f3n: chunks de {chunk_days} d\u00edas\")\n    \n    while current_date < end_dt:\n        chunk_end = min(current_date + timedelta(days=chunk_days), end_dt)\n        \n        # Formatear fechas para QuickBooks (sin microsegundos)\n        chunk_start_str = current_date.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n        chunk_end_str = chunk_end.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n        \n        print(f\"\\n--- CHUNK {chunk_number} ---\")\n        print(f\"Procesando: {chunk_start_str} a {chunk_end_str}\")\n        \n        chunk_start_time = time.time()\n        \n        try:\n            # Query con filtro de fechas - QB usa LastUpdatedTime\n            query = f\"SELECT * FROM Customer WHERE Metadata.LastUpdatedTime >= '{chunk_start_str}' AND Metadata.LastUpdatedTime < '{chunk_end_str}'\"\n            \n            # Ahora con paginaci\u00f3n\n            customers_in_chunk = fetch_customers_with_pagination(\n                realm_id, access_token, base_url, minor_version, chunk_start_str, chunk_end_str\n                )\n\n            \n            # Agregar metadata del chunk a cada customer\n            for customer in customers_in_chunk:\n                customer['_chunk_metadata'] = {\n                    'chunk_number': chunk_number,\n                    'chunk_start': chunk_start_str,\n                    'chunk_end': chunk_end_str,\n                    'processed_at': datetime.utcnow().isoformat()\n                }\n            \n            all_customers.extend(customers_in_chunk)\n            \n            chunk_duration = time.time() - chunk_start_time\n            \n        \n            log_entry = {\n                \"chunk_number\": chunk_number,\n                \"fecha_inicio_chunk\": chunk_start_str,\n                \"fecha_fin_chunk\": chunk_end_str,\n                \"paginas_leidas\": 1,  # QB no usa paginaci\u00f3n tradicional, usar 1\n                \"filas_procesadas\": len(customers_in_chunk),\n                \"duracion_segundos\": round(chunk_duration, 2),\n                \"status\": \"success\",\n                \"timestamp\": datetime.utcnow().isoformat()\n            }\n            \n            processing_log.append(log_entry)\n            \n            print(f\"Chunk {chunk_number} completado:\")\n            print(f\"   - Customers: {len(customers_in_chunk)}\")\n            print(f\"   - Duraci\u00f3n: {chunk_duration:.2f}s\")\n            \n        except Exception as e:\n            error_duration = time.time() - chunk_start_time\n            \n            log_entry = {\n                \"chunk_number\": chunk_number,\n                \"fecha_inicio_chunk\": chunk_start_str,\n                \"fecha_fin_chunk\": chunk_end_str,\n                \"paginas_leidas\": 0,\n                \"filas_procesadas\": 0,\n                \"duracion_segundos\": round(error_duration, 2),\n                \"status\": \"error\",\n                \"error_message\": str(e),\n                \"timestamp\": datetime.utcnow().isoformat()\n            }\n            \n            processing_log.append(log_entry)\n            \n            print(f\"Error en chunk {chunk_number}: {str(e)}\")\n            # Continuar con siguiente chunk en lugar de fallar todo\n        \n        # Siguiente chunk\n        current_date = chunk_end\n        chunk_number += 1\n        \n        # Pausa entre chunks para no sobrecargar API\n        time.sleep(1) \n    \n\n    total_customers = len(all_customers)\n    successful_chunks = len([log for log in processing_log if log['status'] == 'success'])\n    failed_chunks = len([log for log in processing_log if log['status'] == 'error'])\n    total_duration = sum([log['duracion_segundos'] for log in processing_log])\n    \n    print(f\"\\n=== RESUMEN BACKFILL ===\")\n    print(f\"Total customers procesados: {total_customers}\")\n    print(f\"Chunks exitosos: {successful_chunks}\")\n    print(f\"Chunks fallidos: {failed_chunks}\")\n    print(f\"Duraci\u00f3n total: {total_duration:.2f}s\")\n    \n    return {\n        \"QueryResponse\": {\n            \"Customer\": all_customers\n        },\n        \"_processing_log\": processing_log,\n        \"_backfill_summary\": {\n            \"total_customers\": total_customers,\n            \"successful_chunks\": successful_chunks,\n            \"failed_chunks\": failed_chunks,\n            \"total_duration\": total_duration,\n            \"fecha_inicio\": fecha_inicio,\n            \"fecha_fin\": fecha_fin\n        }\n    }\n\n#funcion de carga de datos dentro del mage \n@data_loader\ndef load_data(*args, **kwargs):\n    \n    realm_id = get_secret_value('qb_realm_id')\n    access_token = get_access_token()\n    minor_version = 75 \n    base_url = 'https://sandbox-quickbooks.api.intuit.com' \n    \n    # VERIFICAR SI ES BACKFILL CON PAR\u00c1METROS\n    global_vars = kwargs.get('global_vars', {})\n    fecha_inicio = kwargs.get('fecha_inicio') or global_vars.get('fecha_inicio')\n    fecha_fin = kwargs.get('fecha_fin') or global_vars.get('fecha_fin')\n    chunk_days = kwargs.get('chunk_days') or global_vars.get('chunk_days', 7)\n    \n    if fecha_inicio and fecha_fin:\n        # MODO BACKFILL CON SEGMENTACI\u00d3N\n        print(\"Modo: BACKFILL con par\u00e1metros de fecha\")\n        chunk_days = kwargs.get('chunk_days', 7)  # Default: 1 semana por chunk\n        \n        data = process_backfill_chunks(\n            realm_id, \n            access_token, \n            base_url, \n            minor_version, \n            fecha_inicio, \n            fecha_fin, \n            chunk_days\n        )\n        \n        # Agregar metadata global\n        data['_extraction_metadata'] = {\n            'extraction_type': 'backfill',\n            'fecha_inicio': fecha_inicio,\n            'fecha_fin': fecha_fin,\n            'chunk_days': chunk_days,\n            'extracted_at': datetime.utcnow().isoformat()\n        }\n        \n    else:\n        # MODO TRADICIONAL - extraer todos los customers\n        print(\"Modo: EXTRACCI\u00d3N COMPLETA (sin filtros de fecha)\")\n        query = 'SELECT * FROM Customer' \n        data = _fetch_qb_data(realm_id, access_token, query, base_url, minor_version)\n        \n        # Agregar metadata para mantener consistencia\n        data['_extraction_metadata'] = {\n            'extraction_type': 'full',\n            'extracted_at': datetime.utcnow().isoformat()\n        }\n    \n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'El output est\u00e1 vac\u00edo'\n    \n    # Test espec\u00edfico para backfill\n    if '_processing_log' in output:\n        assert len(output['_processing_log']) > 0, 'No hay logs de procesamiento'\n        print(f\"Chunks procesados: {len(output['_processing_log'])}\")\n    \n    query_response = output.get(\"QueryResponse\", {})\n    customers = query_response.get(\"Customer\", [])\n    print(f\"Total customers extra\u00eddos: {len(customers)}\")\n", "file_path": "data_loaders/ingest_qb_customers.py", "language": "python", "type": "data_loader", "uuid": "ingest_qb_customers"}, "data_loaders/ingest_qb_invoices.py:data_loader:python:ingest qb invoices": {"content": "# librer\u00edas\nimport requests\nimport time\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime, timedelta\nimport json\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n# funci\u00f3n para jalar datos de la API\ndef _fetch_qb_data(realm_id, access_token, query, base_url, minor_version):\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere una URL base y el minor_version\")\n    if not realm_id or not access_token:\n        raise ValueError(\"Se requiere un realm_id y un access_token\")\n\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n    params = {\n        'query': query,\n        'minorversion': minor_version\n    }\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n\n    # Reintentos\n    max_retries = 5\n    for i in range(max_retries):\n        try:\n            print(f'Request a la API: {url}\\nQuery: {query}')\n            response = requests.get(url, headers=headers, params=params, timeout=60)\n\n            if response.status_code == 200:  # \u00e9xito\n                data = response.json()\n                print('Datos recibidos de la API correctamente')\n                return data\n\n            elif response.status_code in [429, 500, 502, 503, 504]:  # errores temporales\n                print(f\"Error {response.status_code}, reintentando ({i+1}/{max_retries})...\")\n                time.sleep(2 ** i)  # backoff exponencial\n                continue\n            else:\n                response.raise_for_status()\n\n        except requests.exceptions.RequestException as e:\n            print(f\"Error en la pull de la API: {e}\")\n            time.sleep(2 ** i)\n\n    raise Exception(f\"Request fall\u00f3 despu\u00e9s de {max_retries} reintentos\")\n\n\n# obtiene un access_token fresco\ndef get_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\n        \"Accept\": \"application/json\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\"\n    }\n    auth = (client_id, client_secret)\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token\n    }\n\n    response = requests.post(url, headers=headers, data=data, auth=auth)\n    response.raise_for_status()\n    token_data = response.json()\n    return token_data['access_token']\n\n\n# paginaci\u00f3n para facturas\ndef fetch_invoices_with_pagination(\n    realm_id, access_token, base_url, minor_version,\n    chunk_start_str, chunk_end_str, page_size=1000\n):\n    invoices = []\n    start_position = 1\n\n    while True:\n        query = (\n            f\"SELECT * FROM Invoice \"\n            f\"WHERE Metadata.LastUpdatedTime >= '{chunk_start_str}' \"\n            f\"AND Metadata.LastUpdatedTime < '{chunk_end_str}' \"\n            f\"STARTPOSITION {start_position} MAXRESULTS {page_size}\"\n        )\n        data = _fetch_qb_data(realm_id, access_token, query, base_url, minor_version)\n        batch = data.get(\"QueryResponse\", {}).get(\"Invoice\", [])\n        if not batch:\n            break\n\n        invoices.extend(batch)\n        start_position += page_size\n\n        if len(batch) < page_size:  # \u00faltima p\u00e1gina\n            break\n\n    return invoices\n\n\n# procesa chunks de backfill\ndef process_backfill_chunks(realm_id, access_token, base_url, minor_version,\n                            fecha_inicio, fecha_fin, chunk_days=7):\n    start_dt = datetime.fromisoformat(fecha_inicio.replace('Z', '+00:00'))\n    end_dt = datetime.fromisoformat(fecha_fin.replace('Z', '+00:00'))\n\n    all_invoices = []\n    processing_log = []\n    current_date = start_dt\n    chunk_number = 1\n\n    print(f\"Iniciando backfill de Invoice desde {fecha_inicio} hasta {fecha_fin}\")\n    print(f\"Segmentaci\u00f3n: chunks de {chunk_days} d\u00edas\")\n\n    while current_date < end_dt:\n        chunk_end = min(current_date + timedelta(days=chunk_days), end_dt)\n        chunk_start_str = current_date.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n        chunk_end_str = chunk_end.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n\n        print(f\"\\n--- CHUNK {chunk_number} ---\")\n        print(f\"Procesando: {chunk_start_str} a {chunk_end_str}\")\n\n        chunk_start_time = time.time()\n        try:\n            invoices_in_chunk = fetch_invoices_with_pagination(\n                realm_id, access_token, base_url, minor_version,\n                chunk_start_str, chunk_end_str\n            )\n\n            for invoice in invoices_in_chunk:\n                invoice['_chunk_metadata'] = {\n                    'chunk_number': chunk_number,\n                    'chunk_start': chunk_start_str,\n                    'chunk_end': chunk_end_str,\n                    'processed_at': datetime.utcnow().isoformat()\n                }\n            all_invoices.extend(invoices_in_chunk)\n\n            chunk_duration = time.time() - chunk_start_time\n            log_entry = {\n                \"chunk_number\": chunk_number,\n                \"fecha_inicio_chunk\": chunk_start_str,\n                \"fecha_fin_chunk\": chunk_end_str,\n                \"paginas_leidas\": 1,\n                \"filas_procesadas\": len(invoices_in_chunk),\n                \"duracion_segundos\": round(chunk_duration, 2),\n                \"status\": \"success\",\n                \"timestamp\": datetime.utcnow().isoformat()\n            }\n            processing_log.append(log_entry)\n\n            print(f\"Chunk {chunk_number} completado:\")\n            print(f\" - Invoices: {len(invoices_in_chunk)}\")\n            print(f\" - Duraci\u00f3n: {chunk_duration:.2f}s\")\n\n        except Exception as e:\n            error_duration = time.time() - chunk_start_time\n            log_entry = {\n                \"chunk_number\": chunk_number,\n                \"fecha_inicio_chunk\": chunk_start_str,\n                \"fecha_fin_chunk\": chunk_end_str,\n                \"paginas_leidas\": 0,\n                \"filas_procesadas\": 0,\n                \"duracion_segundos\": round(error_duration, 2),\n                \"status\": \"error\",\n                \"error_message\": str(e),\n                \"timestamp\": datetime.utcnow().isoformat()\n            }\n            processing_log.append(log_entry)\n            print(f\"Error en chunk {chunk_number}: {str(e)}\")\n\n        current_date = chunk_end\n        chunk_number += 1\n        time.sleep(1)  # pausa entre chunks\n\n    total_invoices = len(all_invoices)\n    successful_chunks = len([log for log in processing_log if log['status'] == 'success'])\n    failed_chunks = len([log for log in processing_log if log['status'] == 'error'])\n    total_duration = sum([log['duracion_segundos'] for log in processing_log])\n\n    print(f\"\\n=== RESUMEN BACKFILL INVOICE ===\")\n    print(f\"Total invoices procesados: {total_invoices}\")\n    print(f\"Chunks exitosos: {successful_chunks}\")\n    print(f\"Chunks fallidos: {failed_chunks}\")\n    print(f\"Duraci\u00f3n total: {total_duration:.2f}s\")\n\n    return {\n        \"QueryResponse\": {\"Invoice\": all_invoices},\n        \"_processing_log\": processing_log,\n        \"_backfill_summary\": {\n            \"total_invoices\": total_invoices,\n            \"successful_chunks\": successful_chunks,\n            \"failed_chunks\": failed_chunks,\n            \"total_duration\": total_duration,\n            \"fecha_inicio\": fecha_inicio,\n            \"fecha_fin\": fecha_fin\n        }\n    }\n\n\n# loader de datos de la tuber\u00eda Mage\n@data_loader\ndef load_data(*args, **kwargs):\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = get_access_token()\n    minor_version = 75\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n\n    fecha_inicio = kwargs.get('fecha_inicio')\n    fecha_fin = kwargs.get('fecha_fin')\n\n    if fecha_inicio and fecha_fin:\n        print(\"Modo: BACKFILL con par\u00e1metros de fecha para Invoice\")\n        chunk_days = kwargs.get('chunk_days', 7)\n\n        data = process_backfill_chunks(\n            realm_id, access_token, base_url, minor_version,\n            fecha_inicio, fecha_fin, chunk_days\n        )\n        data['_extraction_metadata'] = {\n            'extraction_type': 'backfill',\n            'entity': 'Invoice',\n            'fecha_inicio': fecha_inicio,\n            'fecha_fin': fecha_fin,\n            'chunk_days': chunk_days,\n            'extracted_at': datetime.utcnow().isoformat()\n        }\n    else:\n        print(\"Modo: EXTRACCI\u00d3N COMPLETA de Invoice (sin filtros de fecha)\")\n        query = 'SELECT * FROM Invoice'\n        data = _fetch_qb_data(realm_id, access_token, query, base_url, minor_version)\n        data['_extraction_metadata'] = {\n            'extraction_type': 'full',\n            'entity': 'Invoice',\n            'extracted_at': datetime.utcnow().isoformat()\n        }\n\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'El output est\u00e1 vac\u00edo'\n\n    if '_processing_log' in output:\n        assert len(output['_processing_log']) > 0, 'No hay logs de procesamiento'\n        print(f\"Chunks procesados: {len(output['_processing_log'])}\")\n\n    query_response = output.get(\"QueryResponse\", {})\n    invoices = query_response.get(\"Invoice\", [])\n    print(f\"Total invoices extra\u00eddos: {len(invoices)}\")\n", "file_path": "data_loaders/ingest_qb_invoices.py", "language": "python", "type": "data_loader", "uuid": "ingest_qb_invoices"}, "data_loaders/ingest_qb_items.py:data_loader:python:ingest qb items": {"content": "#libreria para hacer peticiones http\nimport requests\nimport time\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime, timedelta\nimport json\n\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n#funcion para jalar datos de la API (anonima, no aparece)\ndef _fetch_qb_data(realm_id, access_token, query, base_url, minor_version):\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere una URL base y el minor_version\")\n\n    if not realm_id or not access_token:\n        raise ValueError(\"Se requiere un realm_id y un access_token\")\n\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n\n    params = {\n        'query': query,\n        'minorversion': minor_version\n    }\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n\n    #Reintentos\n    max_retries = 5\n    for i in range(max_retries):\n        try:\n            print(f'Request a la API: {url}\\nQuery: {query}')\n            response = requests.get(url, headers=headers, params=params, timeout=60)\n\n            #respuestas exitosas\n            if response.status_code == 200:\n                data = response.json()\n                print('Datos recividos de la API correctamente')\n                return data\n\n            # deteccion de errores\n            elif response.status_code in [429, 500, 502, 503, 504]:\n                print(f\"Error {response.status_code}, reintentando ({i+1}/{max_retries})...\")\n                time.sleep(2 ** i)  # backoff exponencial\n                continue\n\n            else:\n                #si hay un error levanta la excepcion\n                response.raise_for_status()\n\n         #Error que nos da de la excepcion\n        except requests.exceptions.RequestException as e:\n            print(f\"Error en la pull de la API: {e}\")\n            time.sleep(2 ** i)\n\n    raise Exception(f\"Request fall\u00f3 despu\u00e9s de {max_retries} reintentos\")\n\ndef get_access_token():\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    refresh_token = get_secret_value('qb_refresh_token')\n\n    url = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n    headers = {\n        \"Accept\": \"application/json\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\"\n    }\n    auth = (client_id, client_secret)\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"refresh_token\": refresh_token\n    }\n\n    response = requests.post(url, headers=headers, data=data, auth=auth)\n    response.raise_for_status()\n    token_data = response.json()\n\n    #access_token fresco\n    return token_data['access_token']\n\ndef fetch_items_with_pagination(realm_id, access_token, base_url, minor_version, chunk_start_str, chunk_end_str, page_size=1000):\n    items = []\n    start_position = 1\n\n    while True:\n        query = (\n            f\"SELECT * FROM Item \"\n            f\"WHERE Metadata.LastUpdatedTime >= '{chunk_start_str}' \"\n            f\"AND Metadata.LastUpdatedTime < '{chunk_end_str}' \"\n            f\"STARTPOSITION {start_position} MAXRESULTS {page_size}\"\n        )\n\n        data = _fetch_qb_data(realm_id, access_token, query, base_url, minor_version)\n        batch = data.get(\"QueryResponse\", {}).get(\"Item\", [])\n\n        if not batch:\n            break\n\n        items.extend(batch)\n        start_position += page_size\n\n        # si devuelve menos del l\u00edmite \u2192 no hay m\u00e1s p\u00e1ginas\n        if len(batch) < page_size:\n            break\n\n    return items\n\n\n#  Procesar backfill con segmentaci\u00f3n para Item\ndef process_backfill_chunks(realm_id, access_token, base_url, minor_version, fecha_inicio, fecha_fin, chunk_days=7):\n    \"\"\"\n    Procesa backfill dividido en chunks por fechas para Item\n    \"\"\"\n    start_dt = datetime.fromisoformat(fecha_inicio.replace('Z', '+00:00'))\n    end_dt = datetime.fromisoformat(fecha_fin.replace('Z', '+00:00'))\n    \n    all_items = []\n    processing_log = []\n    \n    current_date = start_dt\n    chunk_number = 1\n    \n    print(f\"Iniciando backfill de Item desde {fecha_inicio} hasta {fecha_fin}\")\n    print(f\"Segmentaci\u00f3n: chunks de {chunk_days} d\u00edas\")\n    \n    while current_date < end_dt:\n        chunk_end = min(current_date + timedelta(days=chunk_days), end_dt)\n        \n        # Formatear fechas para QuickBooks (sin microsegundos)\n        chunk_start_str = current_date.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n        chunk_end_str = chunk_end.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n        \n        print(f\"\\n--- CHUNK {chunk_number} ---\")\n        print(f\"Procesando: {chunk_start_str} a {chunk_end_str}\")\n        \n        chunk_start_time = time.time()\n        \n        try:\n            # Query con filtro de fechas - QB usa LastUpdatedTime para Item\n            # Ahora con paginaci\u00f3n\n            items_in_chunk = fetch_items_with_pagination(\n                realm_id, access_token, base_url, minor_version, chunk_start_str, chunk_end_str\n                )\n\n            \n            # Agregar metadata del chunk a cada item\n            for item in items_in_chunk:\n                item['_chunk_metadata'] = {\n                    'chunk_number': chunk_number,\n                    'chunk_start': chunk_start_str,\n                    'chunk_end': chunk_end_str,\n                    'processed_at': datetime.utcnow().isoformat()\n                }\n            \n            all_items.extend(items_in_chunk)\n            \n            chunk_duration = time.time() - chunk_start_time\n            \n            # REGISTRAR CADA TRAMO\n            log_entry = {\n                \"chunk_number\": chunk_number,\n                \"fecha_inicio_chunk\": chunk_start_str,\n                \"fecha_fin_chunk\": chunk_end_str,\n                \"paginas_leidas\": 1,  # QB no usa paginaci\u00f3n tradicional, usar 1\n                \"filas_procesadas\": len(items_in_chunk),\n                \"duracion_segundos\": round(chunk_duration, 2),\n                \"status\": \"success\",\n                \"timestamp\": datetime.utcnow().isoformat()\n            }\n            \n            processing_log.append(log_entry)\n            \n            print(f\"Chunk {chunk_number} completado:\")\n            print(f\"   - Items: {len(items_in_chunk)}\")\n            print(f\"   - Duraci\u00f3n: {chunk_duration:.2f}s\")\n            \n        except Exception as e:\n            error_duration = time.time() - chunk_start_time\n            \n            log_entry = {\n                \"chunk_number\": chunk_number,\n                \"fecha_inicio_chunk\": chunk_start_str,\n                \"fecha_fin_chunk\": chunk_end_str,\n                \"paginas_leidas\": 0,\n                \"filas_procesadas\": 0,\n                \"duracion_segundos\": round(error_duration, 2),\n                \"status\": \"error\",\n                \"error_message\": str(e),\n                \"timestamp\": datetime.utcnow().isoformat()\n            }\n            \n            processing_log.append(log_entry)\n            \n            print(f\"Error en chunk {chunk_number}: {str(e)}\")\n            # Continuar con siguiente chunk en lugar de fallar todo\n        \n        # Siguiente chunk\n        current_date = chunk_end\n        chunk_number += 1\n        \n        # Pausa entre chunks para no sobrecargar API\n        time.sleep(1)  # 1 segundo entre chunks\n    \n    # RESUMEN FINAL\n    total_items = len(all_items)\n    successful_chunks = len([log for log in processing_log if log['status'] == 'success'])\n    failed_chunks = len([log for log in processing_log if log['status'] == 'error'])\n    total_duration = sum([log['duracion_segundos'] for log in processing_log])\n    \n    print(f\"\\n=== RESUMEN BACKFILL ITEM ===\")\n    print(f\"Total items procesados: {total_items}\")\n    print(f\"Chunks exitosos: {successful_chunks}\")\n    print(f\"Chunks fallidos: {failed_chunks}\")\n    print(f\"Duraci\u00f3n total: {total_duration:.2f}s\")\n    \n    return {\n        \"QueryResponse\": {\n            \"Item\": all_items\n        },\n        \"_processing_log\": processing_log,\n        \"_backfill_summary\": {\n            \"total_items\": total_items,\n            \"successful_chunks\": successful_chunks,\n            \"failed_chunks\": failed_chunks,\n            \"total_duration\": total_duration,\n            \"fecha_inicio\": fecha_inicio,\n            \"fecha_fin\": fecha_fin\n        }\n    }\n\n#funcion de carga de datos dentro del mage \n@data_loader\ndef load_data(*args, **kwargs):\n    \n    realm_id = get_secret_value('qb_realm_id')\n    access_token = get_access_token()\n    minor_version = 75 \n    base_url = 'https://sandbox-quickbooks.api.intuit.com' \n    \n    # VERIFICAR SI ES BACKFILL CON PAR\u00c1METROS\n    fecha_inicio = kwargs.get('fecha_inicio')\n    fecha_fin = kwargs.get('fecha_fin')\n    \n    if fecha_inicio and fecha_fin:\n        # MODO BACKFILL CON SEGMENTACI\u00d3N\n        print(\"Modo: BACKFILL con par\u00e1metros de fecha para Item\")\n        chunk_days = kwargs.get('chunk_days', 7)  # Default: 1 semana por chunk\n        \n        data = process_backfill_chunks(\n            realm_id, \n            access_token, \n            base_url, \n            minor_version, \n            fecha_inicio, \n            fecha_fin, \n            chunk_days\n        )\n        \n        # Agregar metadata global\n        data['_extraction_metadata'] = {\n            'extraction_type': 'backfill',\n            'entity': 'Item',\n            'fecha_inicio': fecha_inicio,\n            'fecha_fin': fecha_fin,\n            'chunk_days': chunk_days,\n            'extracted_at': datetime.utcnow().isoformat()\n        }\n        \n    else:\n        # MODO TRADICIONAL - extraer todos los items\n        print(\"Modo: EXTRACCI\u00d3N COMPLETA de Item (sin filtros de fecha)\")\n        query = 'SELECT * FROM Item' \n        data = _fetch_qb_data(realm_id, access_token, query, base_url, minor_version)\n        \n        # Agregar metadata para mantener consistencia\n        data['_extraction_metadata'] = {\n            'extraction_type': 'full',\n            'entity': 'Item',\n            'extracted_at': datetime.utcnow().isoformat()\n        }\n    \n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'El output est\u00e1 vac\u00edo'\n    \n    # Test espec\u00edfico para backfill\n    if '_processing_log' in output:\n        assert len(output['_processing_log']) > 0, 'No hay logs de procesamiento'\n        print(f\"Chunks procesados: {len(output['_processing_log'])}\")\n    \n    query_response = output.get(\"QueryResponse\", {})\n    items = query_response.get(\"Item\", [])\n    print(f\"Total items extra\u00eddos: {len(items)}\")", "file_path": "data_loaders/ingest_qb_items.py", "language": "python", "type": "data_loader", "uuid": "ingest_qb_items"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/transform_qb_customers.py:transformer:python:transform qb customers": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nimport json\nfrom datetime import datetime\n\n@transformer\ndef transform(data, *args, **kwargs):\n   \n    #Utilizamos el formato especificado en el deber \n\n    # Datos de la API\n    query_response = data.get(\"QueryResponse\", {})\n    customers = query_response.get(\"Customer\", [])\n\n    # Metadata global\n    ingested_at_utc = datetime.utcnow().isoformat()\n    extract_window_start_utc = kwargs.get(\"extract_window_start_utc\", ingested_at_utc)\n    extract_window_end_utc = kwargs.get(\"extract_window_end_utc\", ingested_at_utc)\n    page_number = kwargs.get(\"page_number\", 1)\n    page_size = kwargs.get(\"page_size\", len(customers))\n    request_payload = kwargs.get(\"request_payload\", {})\n\n    rows = []\n    for cust in customers:\n        rows.append({\n            \"id\": cust.get(\"Id\"),\n            \"payload\": json.dumps(cust),  # JSON completo de la entidad\n            \"ingested_at_utc\": ingested_at_utc,\n            \"extract_window_start_utc\": extract_window_start_utc,\n            \"extract_window_end_utc\": extract_window_end_utc,\n            \"page_number\": page_number,\n            \"page_size\": page_size,\n            \"request_payload\": json.dumps(request_payload),\n        })\n\n    df_customers = pd.DataFrame(rows)\n\n    print(f\"Transformados {len(df_customers)} customers en formato raw staging\")\n\n    return {\n        \"qb_customer\": df_customers\n    }\n\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, \"El output est\u00e1 vac\u00edo\"\n    assert \"qb_customer\" in output, \"No se encontr\u00f3 la tabla qb_customer en el output\"\n    print(\"Shape qb_customer:\", output[\"qb_customer\"].shape)", "file_path": "transformers/transform_qb_customers.py", "language": "python", "type": "transformer", "uuid": "transform_qb_customers"}, "transformers/transform_qb_invoices.py:transformer:python:transform qb invoices": {"content": "#Transformamos los datos crudos en tablas \n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nimport json\nfrom datetime import datetime\n\n@transformer\ndef transform(data, *args, **kwargs):\n   \n   #Utilizamos el formato especificado en el deber \n\n    # Datos de la API\n    query_response = data.get(\"QueryResponse\", {})\n    invoices = query_response.get(\"Invoice\", [])\n\n    # Metadata global\n    ingested_at_utc = datetime.utcnow().isoformat()\n    extract_window_start_utc = kwargs.get(\"extract_window_start_utc\", ingested_at_utc)\n    extract_window_end_utc = kwargs.get(\"extract_window_end_utc\", ingested_at_utc)\n    page_number = kwargs.get(\"page_number\", 1)\n    page_size = kwargs.get(\"page_size\", len(invoices))\n    request_payload = kwargs.get(\"request_payload\", {})\n\n    rows = []\n    for inv in invoices:\n        rows.append({\n            \"id\": inv.get(\"Id\"),\n            \"payload\": json.dumps(inv),  # JSON completo de la entidad\n            \"ingested_at_utc\": ingested_at_utc,\n            \"extract_window_start_utc\": extract_window_start_utc,\n            \"extract_window_end_utc\": extract_window_end_utc,\n            \"page_number\": page_number,\n            \"page_size\": page_size,\n            \"request_payload\": json.dumps(request_payload),\n        })\n\n    df_invoices = pd.DataFrame(rows)\n\n    print(f\"Transformadas {len(df_invoices)} invoices en formato raw staging\")\n\n    return {\n        \"qb_invoice\": df_invoices\n    }\n\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, \"El output est\u00e1 vac\u00edo\"\n    assert \"qb_invoice\" in output, \"No se encontr\u00f3 la tabla qb_invoice en el output\"\n    print(\"Shape qb_invoice:\", output[\"qb_invoice\"].shape)", "file_path": "transformers/transform_qb_invoices.py", "language": "python", "type": "transformer", "uuid": "transform_qb_invoices"}, "transformers/transform_qb_items.py:transformer:python:transform qb items": {"content": "#funcion de transformacion de datos \nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nfrom datetime import datetime, timezone\nimport json\n\n@transformer\ndef transform(data, *args, **kwargs):\n \n    #Utilizamos el formato especificado en el deber \n\n    # Datos de la API\n    query_response = data.get(\"QueryResponse\", {})\n    items = query_response.get(\"Item\", [])\n\n    print(f\"N\u00famero de items encontrados: {len(items)}\")\n\n    if not items:\n        print(\"No hay items para procesar\")\n        return {\"qb_item\": pd.DataFrame()}\n\n    # Extraer metadatos \n    extract_window_start_utc = kwargs.get(\"extract_window_start_utc\", datetime.now(timezone.utc).isoformat())\n    extract_window_end_utc = kwargs.get(\"extract_window_end_utc\", datetime.now(timezone.utc).isoformat())\n    page_number = kwargs.get(\"page_number\", 1)\n    page_size = kwargs.get(\"page_size\", len(items))\n    request_payload = kwargs.get(\"request_payload\", {})\n\n    df_items = pd.DataFrame([\n        {\n            #variables determinadas en el pdf de deber\n            \"id\": item.get(\"Id\"),\n            \"payload\": json.dumps(item, default=str),\n            \"ingested_at_utc\": datetime.now(timezone.utc).isoformat(),\n            \"extract_window_start_utc\": extract_window_start_utc,\n            \"extract_window_end_utc\": extract_window_end_utc,\n            \"page_number\": page_number,\n            \"page_size\": page_size,\n            \"request_payload\": json.dumps(request_payload, default=str),\n        }\n        for item in items\n    ])\n\n    print(f\"Items transformados: {len(df_items)} filas, {len(df_items.columns)} columnas\")\n\n    return {\"qb_item\": df_items}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Test para validar que se cre\u00f3 la tabla qb_item correctamente\n    \"\"\"\n    assert output is not None, \"El output est\u00e1 vac\u00edo\"\n    assert \"qb_item\" in output, \"qb_item no encontrado en output\"\n    df = output[\"qb_item\"]\n    assert \"id\" in df.columns, \"Columna id no encontrada\"\n    assert \"payload\" in df.columns, \"Columna payload no encontrada\"\n    print(f\"Test passed - qb_item shape: {df.shape}\")\n", "file_path": "transformers/transform_qb_items.py", "language": "python", "type": "transformer", "uuid": "transform_qb_items"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/ny_taxi/metadata.yaml:pipeline:yaml:ny taxi/metadata": {"content": "created_at: '2025-08-28 20:42:59.821765+00:00'\ndescription: null\nname: ny_taxi\ntags: []\ntype: python\nuuid: ny_taxi\n", "file_path": "pipelines/ny_taxi/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "ny_taxi/metadata"}, "pipelines/ny_taxi/__init__.py:pipeline:python:ny taxi/  init  ": {"content": "", "file_path": "pipelines/ny_taxi/__init__.py", "language": "python", "type": "pipeline", "uuid": "ny_taxi/__init__"}, "pipelines/qb_customers_backfill/metadata.yaml:pipeline:yaml:qb customers backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - transform_qb_customers\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_qb_customers\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_qb_customers\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_source:\n      path: transformers/transform_qb_customers.py\n  downstream_blocks:\n  - export_qb_costumerss\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: transform_qb_customers\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - ingest_qb_customers\n  uuid: transform_qb_customers\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_qb_costumerss\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - transform_qb_customers\n  uuid: export_qb_costumerss\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-10 16:52:45.812814+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_customers_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_customers_backfill\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_customers_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_customers_backfill/metadata"}, "pipelines/qb_customers_backfill/__init__.py:pipeline:python:qb customers backfill/  init  ": {"content": "", "file_path": "pipelines/qb_customers_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_customers_backfill/__init__"}, "pipelines/qb_invoices_backfill/metadata.yaml:pipeline:yaml:qb invoices backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - transform_qb_invoices\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_qb_invoices\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_qb_invoices\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_qb_invoices\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: transform_qb_invoices\n  retry_config: null\n  status: updated\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - ingest_qb_invoices\n  uuid: transform_qb_invoices\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_qb_invoices\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - transform_qb_invoices\n  uuid: export_qb_invoices\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-10 16:52:45.812814+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_invoices_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_invoices_backfill\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_invoices_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_invoices_backfill/metadata"}, "pipelines/qb_invoices_backfill/triggers.yaml:pipeline:yaml:qb invoices backfill/triggers": {"content": "triggers:\n- description: null\n  envs: []\n  last_enabled_at: 2025-09-12 22:10:26.406482\n  name: 'Trigger de una vez '\n  pipeline_uuid: qb_invoices_backfill\n  schedule_interval: '@once'\n  schedule_type: time\n  settings: null\n  sla: null\n  start_time: 2025-09-11 23:00:00\n  status: active\n  token: 3a4b7437b9f74d2292da1fb80c153f80\n  variables:\n    chunk_days: 7\n    fecha_fin: '2025-09-01T00:00:00Z'\n    fecha_inicio: '2025-01-01T00:00:00Z'\n", "file_path": "pipelines/qb_invoices_backfill/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_invoices_backfill/triggers"}, "pipelines/qb_invoices_backfill/__init__.py:pipeline:python:qb invoices backfill/  init  ": {"content": "", "file_path": "pipelines/qb_invoices_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_invoices_backfill/__init__"}, "pipelines/qb_items_backfill/metadata.yaml:pipeline:yaml:qb items backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - transform_qb_items\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_qb_items\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_qb_items\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_qb_items\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: transform_qb_items\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - ingest_qb_items\n  uuid: transform_qb_items\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_qb_items\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - transform_qb_items\n  uuid: export_qb_items\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-10 16:52:45.812814+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_items_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_items_backfill\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_items_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_items_backfill/metadata"}, "pipelines/qb_items_backfill/__init__.py:pipeline:python:qb items backfill/  init  ": {"content": "", "file_path": "pipelines/qb_items_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_items_backfill/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}